---
title: "Weight_lifting_exercise_assignment"
author: "kannanthegreat"
date: "December 6, 2016"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE, fig.width=10, fig.height=5)
options(width=120)
```
## Executive Summary

This document is the final report of the Peer Assessment project from Coursera's course Practical Machine Learning, as part of the Specialization in Data Science. It was built up in RStudio, using its knitr functions, meant to be published in html format.
Quantified Self devices are becoming more and more common, and are able to collect a large amount of data about people and their personal health activities. The focus of this project is to utilize some sample data on the quality of certain exercises to predict the manner in which they did the exercise.
This analysis is meant to be the basis for the course quiz and a prediction assignment writeup. The main goal of the project is to predict the manner in which 6 participants performed some exercise as described below. 
Three ML models, namely, Decision Tree, GBM and Randon Forest algorithms were considered among which Random forest was found to be most accurate, hence was applied to test the cases. 



## Steps


- Process the data
- Explore the data
- Model selection
- Model cross validation
- Determining expected out of sample error
- Using prediction model to predict 20 different test cases.

## Data Source

The training data for this project are available at:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available at:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv


The data for this project come from : http://groupware.les.inf.puc-rio.br/har. 

## Data Loading and Exploratory Analysis

The following Libraries were used for this project:

```{r}


library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
library(ggplot2)
library(gbm)
library(survival)
library(splines)
library(parallel)
library(plyr)
```
## Loading and exploring Data: 
##### The analysis starts by downloading the data into local files. There are 2 data sets, the training data set and the testing data set. The data exploration reveals many NAs in both data sets. When the data is loaded into dataframes, it is necessary to locate strings containing '#DIV/0!' in otherwise numeric data, a common sentinal error code for division by zero errors. These error codes are loaded into the data frame as NA fields. 
```{r}
set.seed(12345)
training <- read.csv(("C:/Users/kannanthegreat/Documents/Data_train/pml-training.csv"), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(("C:/Users/kannanthegreat/Documents/Data_test/pml-testing.csv"), na.strings=c("NA","#DIV/0!",""))
```

```{r}
str(training)
str(testing)
```
## Partioning Training data set into two data sets: 
### To find an optimal model, with the best performance both in Accuracy as well as minimizing Out of Sample Error, the full testing data is split randomly with a set seed with 60% of the data into the training sample and 40% of the data used as cross-validation.
```{r}
inTrain <- createDataPartition(y=training$classe, p=0.6, list=FALSE)
myTraining <- training[inTrain, ]; myTesting <- training[-inTrain, ]
dim(myTraining); 
dim(myTesting)
```


## Cleaning the data: Transformation 1 - Cleaning Near Zero Variance Variables 

```{r}
myDataNZV <- nearZeroVar(myTraining, saveMetrics=TRUE)
```
## Create another subset without Near Zero Variance variables:
```{r}
myNZVvars <- names(myTraining) %in% c("new_window", "kurtosis_roll_belt", "kurtosis_picth_belt",
                                      "kurtosis_yaw_belt", "skewness_roll_belt", "skewness_roll_belt.1", "skewness_yaw_belt",
                                      "max_yaw_belt", "min_yaw_belt", "amplitude_yaw_belt", "avg_roll_arm", "stddev_roll_arm",
                                      "var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm", "var_pitch_arm", "avg_yaw_arm",
                                      "stddev_yaw_arm", "var_yaw_arm", "kurtosis_roll_arm", "kurtosis_picth_arm",
                                      "kurtosis_yaw_arm", "skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm",
                                      "max_roll_arm", "min_roll_arm", "min_pitch_arm", "amplitude_roll_arm", "amplitude_pitch_arm",
                                      "kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell", "skewness_roll_dumbbell",
                                      "skewness_pitch_dumbbell", "skewness_yaw_dumbbell", "max_yaw_dumbbell", "min_yaw_dumbbell",
                                      "amplitude_yaw_dumbbell", "kurtosis_roll_forearm", "kurtosis_picth_forearm", "kurtosis_yaw_forearm",
                                      "skewness_roll_forearm", "skewness_pitch_forearm", "skewness_yaw_forearm", "max_roll_forearm",
                                      "max_yaw_forearm", "min_roll_forearm", "min_yaw_forearm", "amplitude_roll_forearm",
                                      "amplitude_yaw_forearm", "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm",
                                      "avg_pitch_forearm", "stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm",
                                      "stddev_yaw_forearm", "var_yaw_forearm")
myTraining <- myTraining[!myNZVvars]
#To check the new N?? of observations
dim(myTraining)
```

## Transformation 2: 
### Removing first column of Dataset (ID) it does not interfer with ML Algorithms

```{r}
myTraining <- myTraining[c(-1)]
```
## Transformation 3: 
### Cleaning Variables with too many NAs. For Variables that have more than a 60% threshold of NA's I'm going to leave them out

```{r}
trainingV3 <- myTraining #creating another subset to iterate in loop
for(i in 1:length(myTraining)) { #for every column in the training dataset
        if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .6 ) { #if n?? NAs > 60% of total observations
                for(j in 1:length(trainingV3)) {
                        if( length( grep(names(myTraining[i]), names(trainingV3)[j]) ) ==1)  { #if the columns are the same:
                                trainingV3 <- trainingV3[ , -j] #Remove that column
                        }   
                } 
        }
}
#To check the new N?? of observations
dim(trainingV3)
```
### Applying transformations to "myTesting" and "testing" data sets

```{r}
myTraining <- trainingV3
rm(trainingV3)
clean1 <- colnames(myTraining)
clean2 <- colnames(myTraining[, -58])
myTesting <- myTesting[clean1]
testing <- testing[clean2]
dim(myTesting)
dim(testing)

```

### In order to ensure proper functioning of Decision Trees and especially RandomForest Algorithm with the Test data set (data set provided), we need to coerce the data into the same type.

```{r}
for (i in 1:length(testing) ) {
        for(j in 1:length(myTraining)) {
        if( length( grep(names(myTraining[i]), names(testing)[j]) ) ==1)  {
            class(testing[j]) <- class(myTraining[i])
        }      
    }      
}
#And to make sure Coertion really worked, simple smart ass technique:
testing <- rbind(myTraining[2, -58] , testing) #note removing row 2 as it does not mean anything
testing <- testing[-1,]
```
## Using ML algorithms for prediction: Decision Tree
```{r}
modFitA1 <- rpart(classe ~ ., data=myTraining, method="class")
fancyRpartPlot(modFitA1)
```
## Predicting and Using confusion Matrix to test results:

```{r}
predictionsA1 <- predict(modFitA1, myTesting, type = "class")
cmtree <- confusionMatrix(predictionsA1, myTesting$classe)
cmtree
```
```{r}
plot(cmtree$table, col = cmtree$byClass, main = paste("Decision Tree Confusion Matrix: Accuracy =", round(cmtree$overall['Accuracy'], 4)))
```
## Using ML algorithms for prediction: Random Forests
### Predicting in-sample error and Using confusion Matrix to test results:
```{r}
set.seed(12345)
modFitB1 <- randomForest(classe ~ ., data=myTraining)
predictionB1 <- predict(modFitB1, myTesting, type = "class")
cmrf <- confusionMatrix(predictionB1, myTesting$classe)
cmrf
```
```{r}
plot(modFitB1)
```
```{r}
plot(cmrf$table, col = cmtree$byClass, main = paste("Random Forest Confusion Matrix: Accuracy =", round(cmrf$overall['Accuracy'], 4)))
```
## Prediction with Generalized Boosted Regression

```{r}
set.seed(12345)

fitControl <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 1)

gbmFit1 <- train(classe ~ ., data=myTraining, method = "gbm",
                 trControl = fitControl,
                 verbose = FALSE)


gbmFinMod1 <- gbmFit1$finalModel

gbmPredTest <- predict(gbmFit1, newdata=myTesting)
gbmAccuracyTest <- confusionMatrix(gbmPredTest, myTesting$classe)
gbmAccuracyTest
```
```{r}
plot(gbmFit1, ylim=c(0.9, 1))

```
## Predicting Results on the Test Data
Random Forests gave an Accuracy in the myTesting dataset of 99.89%, which was more accurate that what I got from the Decision Trees or GBM. The expected out-of-sample error is 100-99.89 = 0.11%.

## Generating Files to submit as answers for the Assignment

```{r}
predictionB2 <- predict(modFitB1, testing, type = "class")
predictionB2
```
### Write the results to a text file for submission
```{r}
predictionsB2 <- predict(modFitB1, testing, type = "class")
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(predictionsB2)
```
```{r}
predictionsB2
```
